{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rm-2278/DeadlineFinder/blob/main/dl_basic_2025_competition_vqa_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrgnykefNSGU"
      },
      "source": [
        "# Deep Learning 基礎講座　最終課題: VQA\n",
        "\n",
        "## 概要\n",
        "画像と質問から，回答を予測するタスクです．\n",
        "- サンプル数: 訓練 19,873 サンプル，テスト 4,969 サンプル\n",
        "- 入力: 画像データ（RGB，サイズは画像によって異なります），質問文（系列長はサンプルごとに異なります）\n",
        "- 出力: 回答文（系列長はサンプルごとに異なります）\n",
        "- 評価指標: VQA での評価指標（[こちら\n",
        "](https://visualqa.org/evaluation.html)を参照）を利用しています．\n",
        "\n",
        "### データセット ([VizWiz 2023 edition](https://www.kaggle.com/datasets/nqa112/vizwiz-2023-edition)) の詳細\n",
        "- 24,842 枚の画像データセットと，各画像に対する 1 つの質問文と 10 人の回答者による回答文から構成されます．\n",
        "  - 10 人の回答は全て同じとは限りません．\n",
        "- 24.842 サンプルのうち，80 % (19.873) が訓練データ (train)，20 % (4969) がテストデータ (val) として与えられます．\n",
        "  - テストデータに対する回答文を正解ラベルとし，配布していません．\n",
        "  - データ提供元とは異なるデータ分割になっています．\n",
        "\n",
        "### タスクの詳細\n",
        "- 本コンペティションでは，与えられた画像と質問文に対して，適切な回答文を出力するモデルを作成していただきます．\n",
        "- 評価は [VQA](https://visualqa.org/index.html) (Visual Question Answering) に基づいて，以下の式で計算されます．\n",
        "\n",
        "$$\\text{Acc}(ans) = \\text{min}(\\frac{humans \\; that \\; said \\; ans}{3}, 1)$$\n",
        "\n",
        "- 1 つのデータに対し， 10 人の回答のうち 9 人の回答を選択し上記の式で性能評価した， 10 パターンの Acc の平均をそのデータに対する Acc とします．\n",
        "- 予測結果と正解ラベルを比較する前に，回答を lowercase にする，冠詞は削除するなどの前処理を行っています（[詳細](https://visualqa.org/evaluation.html)）．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3gJ3WWqNRTJ"
      },
      "source": [
        "## 考えられる工夫の例\n",
        "- 事前学習モデルの fine-tuning\n",
        "    - 画像特徴量，言語特徴量を取得するときに，事前学習モデルを fine-tuning することで性能向上が見込めます（今回のタスクと大きく異なるデータセットでの事前学習では効果が小さい可能性がありますので注意しましょう）．\n",
        "- 質問文の表現\n",
        "    - ベースラインでは，質問文をモデルに入力する際に，one-hot ベクトルにしています．これを tokenizer 等を利用して分散表現にすることで，モデル学習しやすくなります．\n",
        "- ソフトラベルの利用\n",
        "    - ベースラインでは 10 人の回答の中で最も多かった回答を正解ラベルとして訓練しています．この点を各回答の頻度に合わせてソフトラベルを利用することで，より多くの情報を利用して学習が可能になります．\n",
        "- 画像の前処理\n",
        "    - 画像の前処理には形状を同じにする Resize のみを利用しています．「畳み込みニューラルネットワーク」，「深層学習と画像認識」等で紹介されていたデータ拡張を追加することで，汎化性能の向上が見込めます．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko1FwYB2qjBp"
      },
      "source": [
        "## 修了要件を満たす条件\n",
        "- ベースラインでは，omnicampus 上での性能評価において， 49.4% となります．したがって，ベースラインを超える 49.4% を超えた提出のみ，修了要件として認めます．\n",
        "- ベースラインから改善を加えることで， 60% に性能向上することを運営で確認しています．こちらを 1 つの指標として取り組んでみてください．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtEm3kflpowF"
      },
      "source": [
        "## 注意点\n",
        "- 最終的な予測モデルは，**配布している訓練データを用いて学習**（ファインチューニング含む）したものとしてください．\n",
        "- 学習を行わず，**事前学習済みモデルの知識のみを利用した推論は禁止**します．  \n",
        "（例: ChatGPT 等の LLM に入力して推論を得るのみ）\n",
        "\n",
        "### 事前学習モデルの利用\n",
        "許可される事項\n",
        "- **構成要素としての事前学習モデルの利用**: 自身で実装したアーキテクチャの一部（特徴抽出，埋め込みなど）として事前学習モデル（BERT，ViT など）を利用することは可能です．\n",
        "- **ファインチューニング**: 上記の用途で利用している事前学習モデルのファインチューニングは可能です．\n",
        "\n",
        "禁止される事項  \n",
        "- **タスク解決用の事前学習モデルの利用**: transformers などで提供されている，対象タスクを直接解くための事前学習モデルでそのまま推論のみ，またはファインチューニングのみで利用することは禁止とします．\n",
        "  - 禁止事項の例: VQA タスクを直接解くための事前学習モデルを VQA タスクで利用する．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YOIezNNg1S-"
      },
      "source": [
        "### データの準備\n",
        "データをダウンロードした際に，google drive したため，利用するために google drive をマウントする必要があります．また， drive 上で展開することができないため，/content ディレクトリ下にコピーし \"data.zip\" を展開します．  \n",
        "google drive 上に \"data.zip\" が配置されていない場合は実行できません．google drive 上に \"data.zip\" (**12GB**) を配置することが可能であれば，\"data_download.ipynb\" を先に実行してください．難しい場合は，omnicampus 演習環境を利用してください．．\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWZZFP4UW0Rw"
      },
      "outputs": [],
      "source": [
        "# omnicampus 上では 4 セル目まで実行不要\n",
        "# ドライブのマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PL8LLFikYN2q"
      },
      "outputs": [],
      "source": [
        "# データダウンロード用の notebook にてgoogle drive への保存後，\n",
        "# 反映に時間がかかる可能性がありますので，google drive のマウント後，\n",
        "# data.zip がディレクトリ内にあることを確認してから実行してください．\n",
        "# data.zip を /content 下にコピーする\n",
        "!cp \"/content/drive/MyDrive/data.zip\" \"/content\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2O7pkQvmZ1wW"
      },
      "outputs": [],
      "source": [
        "# カレントディレクトリ下のファイル群を確認\n",
        "# data.zip が表示されれば問題ないです\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEae3j7d6Iuc"
      },
      "outputs": [],
      "source": [
        "# データを解凍する\n",
        "!unzip data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijF6qw6IwkFh"
      },
      "source": [
        "omnicampus 演習環境では，data_download.ipynb のマウント，zip 化，drive へのコピーを実行しないことで，\"data.zip\" を解凍した形で配置されます．したがって，data ディレクトリが存在するディレクトリをカレントディレクトリとするだけで良いです．\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6ZGrJT7w_wV"
      },
      "outputs": [],
      "source": [
        "# omnicampus 実行用\n",
        "# 以下の例では/workspace に data ディレクトリがあると想定\n",
        "%cd /workspace/VQA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLSfnK-rZuwl"
      },
      "source": [
        "### 1. import library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peG6AR64aD0G"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import random\n",
        "import time\n",
        "from statistics import mode\n",
        "from collections import Counter\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from transformers import BertTokenizer, BertModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YOJqWpSaGGL"
      },
      "source": [
        "### 2. utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKpVLucPaHj4"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    シードを固定する．\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    seed : int\n",
        "        乱数生成に用いるシード値．\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MD2OngWUaPYy"
      },
      "outputs": [],
      "source": [
        "def process_text(text):\n",
        "    \"\"\"\n",
        "    入力文と回答のフォーマットを統一するための関数．\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str\n",
        "        入力文，もしくは回答．\n",
        "    \"\"\"\n",
        "    # lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # 数詞を数字に変換\n",
        "    num_word_to_digit = {\n",
        "        'zero': '0', 'one': '1', 'two': '2', 'three': '3', 'four': '4',\n",
        "        'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9',\n",
        "        'ten': '10'\n",
        "    }\n",
        "    for word, digit in num_word_to_digit.items():\n",
        "        text = text.replace(word, digit)\n",
        "\n",
        "    # 小数点のピリオドを削除\n",
        "    text = re.sub(r'(?<!\\d)\\.(?!\\d)', '', text)\n",
        "\n",
        "    # 冠詞の削除\n",
        "    text = re.sub(r'\\b(a|an|the)\\b', '', text)\n",
        "\n",
        "    # 短縮形のカンマの追加\n",
        "    contractions = {\n",
        "        \"dont\": \"don't\", \"isnt\": \"isn't\", \"arent\": \"aren't\", \"wont\": \"won't\",\n",
        "        \"cant\": \"can't\", \"wouldnt\": \"wouldn't\", \"couldnt\": \"couldn't\"\n",
        "    }\n",
        "    for contraction, correct in contractions.items():\n",
        "        text = text.replace(contraction, correct)\n",
        "\n",
        "    # 句読点をスペースに変換\n",
        "    text = re.sub(r\"[^\\w\\s':]\", ' ', text)\n",
        "\n",
        "    # 句読点をスペースに変換\n",
        "    text = re.sub(r'\\s+,', ',', text)\n",
        "\n",
        "    # 連続するスペースを1つに変換\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tTxwYSKaoOO"
      },
      "outputs": [],
      "source": [
        "class VQADataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    VQA データセットを扱うためのクラス．\n",
        "    Improved version with BERT tokenization and soft labels.\n",
        "    \"\"\"\n",
        "    def __init__(self, df_path, image_dir, transform=None, answer=True, tokenizer=None):\n",
        "        self.transform = transform  # 画像の前処理\n",
        "        self.image_dir = image_dir  # 画像ファイルのディレクトリ\n",
        "        self.df = pandas.read_json(df_path)  # 画像ファイルのパス，question, answerを持つDataFrame\n",
        "        self.answer = answer\n",
        "        self.tokenizer = tokenizer  # BERT tokenizer\n",
        "\n",
        "        # question / answerの辞書を作成\n",
        "        self.question2idx = {}\n",
        "        self.answer2idx = {}\n",
        "        self.idx2question = {}\n",
        "        self.idx2answer = {}\n",
        "\n",
        "        # 質問文に含まれる単語を辞書に追加 (kept for compatibility)\n",
        "        for question in self.df[\"question\"]:\n",
        "            question = process_text(question)\n",
        "            words = question.split(\" \")\n",
        "            for word in words:\n",
        "                if word not in self.question2idx:\n",
        "                    self.question2idx[word] = len(self.question2idx)\n",
        "        self.idx2question = {v: k for k, v in self.question2idx.items()}  # 逆変換用の辞書(question)\n",
        "\n",
        "        if self.answer:\n",
        "            # 回答に含まれる文章を辞書に追加\n",
        "            for answers in self.df[\"answers\"]:\n",
        "                for answer in answers:\n",
        "                    word = answer[\"answer\"]\n",
        "                    word = process_text(word)\n",
        "                    if word not in self.answer2idx:\n",
        "                        self.answer2idx[word] = len(self.answer2idx)\n",
        "            self.idx2answer = {v: k for k, v in self.answer2idx.items()}  # 逆変換用の辞書(answer)\n",
        "\n",
        "    def update_dict(self, dataset):\n",
        "        \"\"\"\n",
        "        検証用データ，テストデータの辞書を訓練データの辞書に更新する．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        dataset : Dataset\n",
        "            訓練データのDataset\n",
        "        \"\"\"\n",
        "        self.question2idx = dataset.question2idx\n",
        "        self.answer2idx = dataset.answer2idx\n",
        "        self.idx2question = dataset.idx2question\n",
        "        self.idx2answer = dataset.idx2answer\n",
        "        self.tokenizer = dataset.tokenizer\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        対応するidxのデータ（画像，質問，回答）を取得．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        idx : int\n",
        "            取得するデータのインデックス\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        image : torch.Tensor  (C, H, W)\n",
        "            画像データ\n",
        "        question_ids : torch.Tensor  (max_length)\n",
        "            BERT tokenized question input ids\n",
        "        question_mask : torch.Tensor  (max_length)\n",
        "            BERT attention mask\n",
        "        answers : torch.Tensor  (n_answer)\n",
        "            10人の回答者の回答のid\n",
        "        target_scores : torch.Tensor  (num_classes)\n",
        "            Soft labels - frequency distribution of answers (0.0 to 1.0)\n",
        "        \"\"\"\n",
        "        image = Image.open(f\"{self.image_dir}/{self.df['image'][idx]}\")\n",
        "        image = self.transform(image)\n",
        "\n",
        "        # Get raw question text for BERT tokenization\n",
        "        question_text = self.df[\"question\"][idx]\n",
        "\n",
        "        # Tokenize with BERT tokenizer\n",
        "        encoding = self.tokenizer(\n",
        "            question_text,\n",
        "            padding='max_length',\n",
        "            max_length=32,\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        question_ids = encoding['input_ids'].squeeze(0)\n",
        "        question_mask = encoding['attention_mask'].squeeze(0)\n",
        "\n",
        "        if self.answer:\n",
        "            answers = [self.answer2idx[process_text(answer[\"answer\"])] for answer in self.df[\"answers\"][idx]]\n",
        "\n",
        "            # Create soft labels based on answer frequency\n",
        "            num_classes = len(self.answer2idx)\n",
        "            target_scores = torch.zeros(num_classes)\n",
        "            answer_counts = Counter(answers)\n",
        "            for ans_idx, count in answer_counts.items():\n",
        "                # VQA score: min(count/3, 1.0)\n",
        "                target_scores[ans_idx] = min(count / 3.0, 1.0)\n",
        "\n",
        "            return image, question_ids, question_mask, torch.Tensor(answers), target_scores\n",
        "\n",
        "        else:\n",
        "            return image, question_ids, question_mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f83T-sQca0DC"
      },
      "outputs": [],
      "source": [
        "def VQA_criterion(batch_pred, batch_answers):\n",
        "    \"\"\"\n",
        "    VQA タスクに用いられる評価関数．\n",
        "    \"\"\"\n",
        "    total_acc = 0.\n",
        "\n",
        "    for pred, answers in zip(batch_pred, batch_answers):\n",
        "        acc = 0.\n",
        "        for i in range(len(answers)):\n",
        "            num_match = 0\n",
        "            for j in range(len(answers)):\n",
        "                if i == j:\n",
        "                    continue\n",
        "                if pred == answers[j]:\n",
        "                    num_match += 1\n",
        "            acc += min(num_match / 3, 1)\n",
        "        total_acc += acc / 10\n",
        "\n",
        "    return total_acc / len(batch_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afULfdGfa9BF"
      },
      "source": [
        "### 3. model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8bOkolIbBBg"
      },
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    ResNet の basic block\n",
        "    \"\"\"\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        \"\"\"\n",
        "        コンストラクタ．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        in_channles: int\n",
        "            入力のチャネル数\n",
        "        out_channels:\n",
        "            出力のチャネル数\n",
        "        stride: int\n",
        "            ストライド\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        順伝播処理\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: torch.Tensor\n",
        "            ブロックへの入力\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        out: torch.Tensor\n",
        "            ブロックへの出力\n",
        "        \"\"\"\n",
        "        residual = x\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "\n",
        "        out += self.shortcut(residual)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class BottleneckBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    ResNet の bottleneck block\n",
        "    \"\"\"\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        \"\"\"\n",
        "        コンストラクタ．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        in_channles: int\n",
        "            入力のチャネル数\n",
        "        out_channels:\n",
        "            出力のチャネル数\n",
        "        stride: int\n",
        "            ストライド\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, stride=1)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels * self.expansion:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels * self.expansion, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm2d(out_channels * self.expansion)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        順伝播処理\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: torch.Tensor\n",
        "            ブロックへの入力\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        out: torch.Tensor\n",
        "            ブロックへの出力\n",
        "        \"\"\"\n",
        "        residual = x\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "\n",
        "        out += self.shortcut(residual)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    \"\"\"\n",
        "    ResNet の実装\n",
        "    \"\"\"\n",
        "    def __init__(self, block, layers):\n",
        "        \"\"\"\n",
        "        コンストラクタ．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        block: torch.nn.Module\n",
        "            利用するブロックのクラス (BasicBlock / BottleneckBlock)\n",
        "        layers: list\n",
        "            各ブロックの層数\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.in_channels = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, layers[0], 64)\n",
        "        self.layer2 = self._make_layer(block, layers[1], 128, stride=2)\n",
        "        self.layer3 = self._make_layer(block, layers[2], 256, stride=2)\n",
        "        self.layer4 = self._make_layer(block, layers[3], 512, stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, 512)\n",
        "\n",
        "    def _make_layer(self, block, blocks, out_channels, stride=1):\n",
        "        \"\"\"\n",
        "        同じ構成を繰り返す部分を生成する．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        block: torch.nn.Module\n",
        "            利用するブロックのクラス (BasicBlock / BottleneckBlock)\n",
        "        blocks: int\n",
        "            層数\n",
        "        out_channels: int\n",
        "            出力のチャネル数\n",
        "        stride: int\n",
        "            ストライド\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        layers: torch.nn.ModuleList\n",
        "            生成した層\n",
        "        \"\"\"\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride))\n",
        "        self.in_channels = out_channels * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.in_channels, out_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        順伝播処理\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: torch.Tensor\n",
        "            入力データ\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x: torch.Tensor\n",
        "            ResNet によって生成される特徴量\n",
        "        \"\"\"\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptR4SOabbG1-"
      },
      "outputs": [],
      "source": [
        "def ResNet18():\n",
        "    \"\"\"\n",
        "    ResNet18 を生成する関数．\n",
        "    \"\"\"\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    \"\"\"\n",
        "    ResNet50 を生成する関数．\n",
        "    \"\"\"\n",
        "    return ResNet(BottleneckBlock, [3, 4, 6, 3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4NR2fhDbID_"
      },
      "outputs": [],
      "source": [
        "class VQAModel(nn.Module):\n",
        "    \"\"\"\n",
        "    VQA タスクを解くためのモデル例．\n",
        "    Improved version with BERT text encoder and attention mechanism.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_answer: int, freeze_bert: bool = True):\n",
        "        \"\"\"\n",
        "        コンストラクタ．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_answer: int\n",
        "            出力のクラス数\n",
        "        freeze_bert: bool\n",
        "            BERT weights を固定するかどうか\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Image encoder: Pre-trained ResNet18 with feature grid output\n",
        "        resnet = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "        # Remove the final avgpool and fc to get spatial features\n",
        "        self.image_encoder = nn.Sequential(*list(resnet.children())[:-2])\n",
        "        self.image_feat_dim = 512  # ResNet18 final conv output channels\n",
        "\n",
        "        # Text encoder: BERT\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.bert_dim = 768\n",
        "\n",
        "        # Freeze BERT weights for stable training\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        # Project BERT dimension to match image features for attention\n",
        "        self.text_proj = nn.Linear(self.bert_dim, self.image_feat_dim)\n",
        "\n",
        "        # Attention mechanism: Query (text), Key/Value (image)\n",
        "        self.attention_scale = self.image_feat_dim ** 0.5\n",
        "\n",
        "        # Fusion and classification layers\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(self.image_feat_dim * 2, 1024)  # fusion_dim = text_proj + attended_image,\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, n_answer)\n",
        "        )\n",
        "\n",
        "    def forward(self, image, question_ids, question_mask):\n",
        "        \"\"\"\n",
        "        順伝播処理\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        image: torch.Tensor (B, C, H, W)\n",
        "            入力画像\n",
        "        question_ids: torch.Tensor (B, seq_len)\n",
        "            BERT tokenized question input ids\n",
        "        question_mask: torch.Tensor (B, seq_len)\n",
        "            BERT attention mask\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        logits: torch.Tensor (B, n_answer)\n",
        "            出力ロジット\n",
        "        \"\"\"\n",
        "        batch_size = image.size(0)\n",
        "\n",
        "        # Extract image features: (B, 512, H', W') where H'=W'=7 for 224x224 input\n",
        "        image_features = self.image_encoder(image)  # (B, 512, 7, 7)\n",
        "        h, w = image_features.size(2), image_features.size(3)\n",
        "\n",
        "        # Reshape to (B, H'*W', 512) for attention\n",
        "        image_features = image_features.view(batch_size, self.image_feat_dim, -1)  # (B, 512, 49)\n",
        "        image_features = image_features.permute(0, 2, 1)  # (B, 49, 512)\n",
        "\n",
        "        # Extract text features using BERT\n",
        "        bert_output = self.bert(input_ids=question_ids, attention_mask=question_mask)\n",
        "        # Use [CLS] token as sentence representation\n",
        "        text_cls = bert_output.last_hidden_state[:, 0, :]  # (B, 768)\n",
        "\n",
        "        # Project text features to image feature dimension\n",
        "        text_proj = self.text_proj(text_cls)  # (B, 512)\n",
        "\n",
        "        # Attention: text as query, image as key/value\n",
        "        # Compute attention scores: (B, 49)\n",
        "        attention_scores = torch.bmm(image_features, text_proj.unsqueeze(2)).squeeze(2)  # (B, 49)\n",
        "        attention_scores = attention_scores / self.attention_scale\n",
        "        attention_weights = F.softmax(attention_scores, dim=1)  # (B, 49)\n",
        "\n",
        "        # Compute attended image features: weighted sum of image features\n",
        "        attended_image = torch.bmm(attention_weights.unsqueeze(1), image_features).squeeze(1)  # (B, 512)\n",
        "\n",
        "        # Concatenate text features and attended image features\n",
        "        fused = torch.cat([text_proj, attended_image], dim=1)  # (B, 1024)\n",
        "\n",
        "        # Classification\n",
        "        logits = self.fusion(fused)  # (B, n_answer)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKFXLwakbMmq"
      },
      "source": [
        "### 4. train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxFHy9ozbNo9"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, optimizer, criterion, device):\n",
        "    \"\"\"\n",
        "    学習用の関数．（Soft labels + BERT対応版）\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: torch.nn.Module\n",
        "        学習するモデル\n",
        "    dataloader: torch.utils.data.DataLoader\n",
        "        学習に利用するデータローダ\n",
        "    optimizer: torch.optim.Optim\n",
        "        最適化手法\n",
        "    criterion: torch.nn.Module\n",
        "        損失関数 (BCEWithLogitsLoss)\n",
        "    device: torch.device\n",
        "        学習に利用するデバイス\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    total_loss: float\n",
        "        平均損失\n",
        "    total_acc: float\n",
        "        平均正解率\n",
        "    simple_acc: float\n",
        "        最頻値に対する正解率（VQA の評価指標とは異なることに注意）\n",
        "    time: float\n",
        "        1 エポックの学習にかかった時間 (sec)\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    simple_acc = 0\n",
        "\n",
        "    start = time.time()\n",
        "    for image, question_ids, question_mask, answers, target_scores in dataloader:\n",
        "        image = image.to(device)\n",
        "        question_ids = question_ids.to(device)\n",
        "        question_mask = question_mask.to(device)\n",
        "        answers = answers.to(device)\n",
        "        target_scores = target_scores.to(device)\n",
        "\n",
        "        pred = model(image, question_ids, question_mask)\n",
        "        loss = criterion(pred, target_scores)  # BCEWithLogitsLoss with soft labels\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # For evaluation metrics, use argmax prediction\n",
        "        pred_idx = pred.argmax(1)\n",
        "        # Get mode answer from answers for simple_acc\n",
        "        mode_answers = torch.mode(answers, dim=1).values.long()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_acc += VQA_criterion(pred_idx, answers)  # VQA accuracy\n",
        "        simple_acc += (pred_idx == mode_answers).float().mean().item()  # simple accuracy\n",
        "\n",
        "    return total_loss / len(dataloader), total_acc / len(dataloader), simple_acc / len(dataloader), time.time() - start\n",
        "\n",
        "\n",
        "def eval(model, dataloader, optimizer, criterion, device):\n",
        "    \"\"\"\n",
        "    評価用の関数．（Soft labels + BERT対応版）\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: torch.nn.Module\n",
        "        モデル\n",
        "    dataloader: torch.utils.data.DataLoader\n",
        "        評価に利用するデータローダ\n",
        "    criterion: torch.nn.Module\n",
        "        損失関数\n",
        "    device: torch.device\n",
        "        利用するデバイス\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    total_loss: float\n",
        "        平均損失\n",
        "    total_acc: float\n",
        "        平均正解率\n",
        "    simple_acc: float\n",
        "        最頻値に対する正解率（VQA の評価指標とは異なることに注意）\n",
        "    time: float\n",
        "        1 エポックの評価にかかった時間 (sec)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    simple_acc = 0\n",
        "\n",
        "    start = time.time()\n",
        "    with torch.no_grad():\n",
        "        for image, question_ids, question_mask, answers, target_scores in dataloader:\n",
        "            image = image.to(device)\n",
        "            question_ids = question_ids.to(device)\n",
        "            question_mask = question_mask.to(device)\n",
        "            answers = answers.to(device)\n",
        "            target_scores = target_scores.to(device)\n",
        "\n",
        "            pred = model(image, question_ids, question_mask)\n",
        "            loss = criterion(pred, target_scores)\n",
        "\n",
        "            # For evaluation metrics, use argmax prediction\n",
        "            pred_idx = pred.argmax(1)\n",
        "            mode_answers = torch.mode(answers, dim=1).values.long()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_acc += VQA_criterion(pred_idx, answers)  # VQA accuracy\n",
        "            simple_acc += (pred_idx == mode_answers).float().mean().item()  # simple accuracy\n",
        "\n",
        "    return total_loss / len(dataloader), total_acc / len(dataloader), simple_acc / len(dataloader), time.time() - start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUbpNdDgbTgP"
      },
      "source": [
        "### 5. make submission file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHF3chhFbkky"
      },
      "outputs": [],
      "source": [
        "# deviceの設定\n",
        "set_seed(42)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Initialize BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# dataloader / model\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
        "])\n",
        "train_dataset = VQADataset(df_path=\"./data/train.json\", image_dir=\"./data/train\", transform=transform, tokenizer=tokenizer)\n",
        "test_dataset = VQADataset(df_path=\"./data/valid.json\", image_dir=\"./data/valid\", transform=transform, answer=False, tokenizer=tokenizer)\n",
        "test_dataset.update_dict(train_dataset)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "model = VQAModel(n_answer=len(train_dataset.answer2idx), freeze_bert=True).to(device)\n",
        "\n",
        "# optimizer / criterion\n",
        "num_epoch = 10\n",
        "criterion = nn.BCEWithLogitsLoss()  # Changed to BCEWithLogitsLoss for soft labels\n",
        "# Only optimize trainable parameters (frozen BERT params excluded)\n",
        "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4, weight_decay=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQopVAWZb1ee"
      },
      "outputs": [],
      "source": [
        "# train model\n",
        "for epoch in range(num_epoch):\n",
        "    train_loss, train_acc, train_simple_acc, train_time = train(model, train_loader, optimizer, criterion, device)\n",
        "    print(f\"【{epoch + 1}/{num_epoch}】\\n\"\n",
        "            f\"train time: {train_time:.2f} [s]\\n\"\n",
        "            f\"train loss: {train_loss:.4f}\\n\"\n",
        "            f\"train acc: {train_acc:.4f}\\n\"\n",
        "            f\"train simple acc: {train_simple_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mD-SpZwMb81q"
      },
      "outputs": [],
      "source": [
        "# make submission file\n",
        "model.eval()\n",
        "submission = []\n",
        "with torch.no_grad():\n",
        "    for image, question_ids, question_mask in test_loader:\n",
        "        image = image.to(device)\n",
        "        question_ids = question_ids.to(device)\n",
        "        question_mask = question_mask.to(device)\n",
        "        pred = model(image, question_ids, question_mask)\n",
        "        pred = pred.argmax(1).cpu().item()\n",
        "        submission.append(pred)\n",
        "\n",
        "submission = [train_dataset.idx2answer[id] for id in submission]\n",
        "submission = np.array(submission)\n",
        "torch.save(model.state_dict(), \"model.pt\")\n",
        "np.save(\"submission.npy\", submission)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbEyP4q9b_6f"
      },
      "source": [
        "## 提出方法\n",
        "\n",
        "以下の3点をzip化し，Omnicampusの「最終課題 (VQA)」から提出してください．\n",
        "\n",
        "- `submission.npy`\n",
        "- `model.pt`や`model_best.pt`など，テストに使用した重み（拡張子は`.pt`のみ）\n",
        "- 本Colab Notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kW3NA_ruqegr"
      },
      "outputs": [],
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "model_path = \"model.pt\"\n",
        "notebook_path = \"/content/drive/MyDrive/Colab Notebooks/DL_Basic_2025_Competition_VQA_baseline.ipynb\"\n",
        "\n",
        "with ZipFile(\"submission.zip\", \"w\") as zf:\n",
        "    zf.write(\"submission.npy\")\n",
        "    zf.write(model_path)\n",
        "    zf.write(notebook_path, arcname=\"DL_Basic_2025_Competition_VQA_baseline.ipynb\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKRPwFaLFw1f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}